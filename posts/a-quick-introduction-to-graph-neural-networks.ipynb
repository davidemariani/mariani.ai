{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4489a196-89e3-48ad-a515-b0be435b186b",
   "metadata": {},
   "source": [
    "*Geometric Deep Learning* <a href=\"https://twitter.com/prlz77/status/1178662575900368903\" target=\"_blank\">has become increasingly popular over the past few years</a>, and my curiosity about it has been growing significantly in the past few months.  \n",
    "In this post, I will try to compress the main intuitions (at a very high level) behind *Graph neural networks* (GNN), so that it will be easier to get to more detailed aspects in the future posts.\n",
    "<!-- TEASER_END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef168ed-7b09-401f-bbdf-ae3f7371cc28",
   "metadata": {},
   "source": [
    "<h3><strong>Table of contents:</strong></h3>\n",
    "\n",
    "1. [What is a graph?](#what-is-a-graph)\n",
    "2. [Node, edge and graph level information](#information-level)\n",
    "3. [Graphs applications](#modelling-examples)\n",
    "4. [Graph Machine Learning Tasks and Challenges](#GML-tasks)\n",
    "5. [Computation Graphs, Message Passing and Aggregation](#computation-graphs)\n",
    "6. [Transductive and inductive learning](#transductive-inductive)\n",
    "7. [Frameworks for GNN](#frameworks)\n",
    "6. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70fbbe-762b-4313-a251-a66e92b11a22",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2><strong>What is a graph?</strong></h2> <a class=\"anchor\" id=\"what-is-a-graph\"></a>\n",
    "    \n",
    "A graph is a data structure made of *nodes (or vertices)* connected by *edges (or links)*; we can simply say that edges represent the relationships between nodes.  \n",
    "Depending on the way the edges link the nodes, we can classify graphs as <a href=\"https://en.wikipedia.org/wiki/Directed_graph\" target=\"_blank\"><em>directed</em></a> (edges direction matters and is specified at each edge) and <a href=\"https://www.cpp.edu/~ftang/courses/CS241/notes/graph.htm\" target=\"_blank\"><em>undirected</em></a> (edges have no specified direction and can be traversed how we desire).  \n",
    "<div align=\"center\">\n",
    "    <img src=\"/images/undirected_directed_graph.png\" alt=\"Drawing\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7820d-86ae-4c6e-80b5-fcf1fd4be8ad",
   "metadata": {},
   "source": [
    "Connectivity is another way to do network classification: for example, a graph in which all the nodes are connected to all the others is called <a href=\"https://en.wikipedia.org/wiki/Complete_graph\" target=\"_blank\"><em>complete</em></a>, while a graph without any directed cylces is categorized as <a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\" target=\"_blank\"><em>directed acyclic</em></a>.\n",
    "<div align=\"center\">\n",
    "    <img src=\"/images/complete_acyclic_graph.png\" alt=\"Drawing\" width=300/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e7a85-4faa-4d1e-9dc0-caeee29067aa",
   "metadata": {},
   "source": [
    "There are many different graph classifications but the important thing to bear in mind is that a graph can commonly be described by its connectivity information with an <a href=\"https://en.wikipedia.org/wiki/Adjacency_matrix#:~:text=In%20graph%20theory%20and%20computer,or%20not%20in%20the%20graph.&text=If%20the%20graph%20is%20undirected,the%20adjacency%20matrix%20is%20symmetric.\" target=\"_blank\"><em>adjacency matrix</em></a>, that simply stores a \"1\" if two nodes are connected (like node \"a\" and node \"c\" in the image below) or a \"0\" if they are not. \n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"/images/adjacency_matrix.png\" alt=\"Drawing\" width=400/> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df91213-57e5-4124-a0c9-b964bc208598",
   "metadata": {},
   "source": [
    "<h2><strong>Node, edge and graph level information</strong></h2> <a class=\"anchor\" id=\"information-level\"></a>   \n",
    "<img align='left' src=\"/images/embedding.png\" alt=\"Drawing\" width=400/>\n",
    "\n",
    "Each piece of the graph carries a different information level: \n",
    "- **node**, with attributes such as number of neighbours or node class \n",
    "- **edge**, with directions and attributes like edge weight or edge class\n",
    "- **graph information**, with global attributes like number of nodes or shortest paths\n",
    "\n",
    "Furthermore, each element can be seen as a place in which we can store information, creating different types of <a href=\"https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture#:~:text=An%20embedding%20is%20a%20relatively,can%20translate%20high%2Ddimensional%20vectors.&text=Ideally%2C%20an%20embedding%20captures%20some,learned%20and%20reused%20across%20models.\" target=\"_blank\">embedding</a>.  \n",
    "This is particularly important in the *deep learning* domain, since embeddings can be learned and treated similarly to weights to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae3c3a-7ab7-4963-90b8-d7ea5a30864c",
   "metadata": {},
   "source": [
    "<h2><strong>Graphs applications</strong></h2> <a class=\"anchor\" id=\"modelling-examples\"></a>\n",
    "\n",
    "As stated in the magnificent article <a href=\"https://distill.pub/2021/gnn-intro/#graph-to-tensor\" target=\"_blank\"><em>A gentle introduction to Graph Neural Networks</em></a>[<sup>1</sup>](#fn1) (from which I have taken most of the inspiration for this post), *\"graphs are all around us\"*, and the followings are just a few examples of what can be represented using them: \n",
    "- **interactions** (like <a href=\"https://brilliant.org/wiki/social-networks/\" target=\"_blank\">social media networks</a>, <a href=\"https://wits.worldbank.org/CountryNetwork.aspx?lang=en\" target=\"_blank\">trade networks</a> or document citation networks. The <a href=\"https://relational.fit.cvut.cz/dataset/CORA\" target=\"_blank\">CORA</a> dataset is a good example of the latter)\n",
    "- **fraud detection systems** (<a href=\"https://blog.careem.com/en/crazywall-graph-based-identity-fraud-detection/\" target=\"_blank\">here</a> a graph based identity fraud detection is described)\n",
    "- **chemical molecular structures** or interactions between proteins (like the <a href=\"https://paperswithcode.com/dataset/ppi\" target=\"_blank\">PPI</a> dataset)\n",
    "- **recommender systems** (here's <a href=\"https://towardsdatascience.com/graph-neural-network-gnn-architectures-for-recommendation-systems-7b9dd0de0856\" target=\"_blank\">a review of GNN for recommender systems</a>)\n",
    "- **manifolds** (an application of geometric deep learning on manifold is shown in <a href=\"https://www.youtube.com/watch?v=-b0e41H4J_A\" target=\"_blank\">this short video</a>)\n",
    "\n",
    "Even **images** and **text** can be represented using a graph!  \n",
    "An image is a graph in which each node represents a pixel, and each edge shows the connection between that pixel and the adjacent ones.  \n",
    "A node representing a non-border pixel (like the central one in the image below) always has 8 linked nodes, and the vector stored within the node has 3 dimensions representing the RGB channel.\n",
    "<div>\n",
    "    <img src=\"/images/image_to_graph.png\" alt=\"Drawing\" width=400/> \n",
    "</div>\n",
    "\n",
    "When dealing with text, <a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\" target=\"_blank\">tokenization</a> is one of the fundamental pre-processing steps, so that we can use characters, words and similar as separate elements of a sequence. Each of these elements can potentially be modelled as a node of a graph.\n",
    "<div>\n",
    "    <img src=\"/images/text_to_graph.png\" alt=\"Drawing\" width=400/> \n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3028d7d-296a-476b-b1eb-86646372b662",
   "metadata": {},
   "source": [
    "<h2><strong>Graph Machine Learning: Tasks and Challenges</strong></h2> <a class=\"anchor\" id=\"GML-tasks\"></a>\n",
    "\n",
    "The types of prediction that we can do when working on a graph happen at the usual three main levels:  \n",
    "\n",
    "- **Graph level** predictions (where we try to predict a property of the entire graph, and the label is assigned to the entire graph itself)\n",
    "- **Node level** predictions (where we want to predict the role or identity of a node. A typical example is the <a href=\"https://en.wikipedia.org/wiki/Zachary%27s_karate_club\" target=\"_blank\">Zachary's Karate Club</a>)\n",
    "- **Edge level** predictions (where we make predictions on the relationships between the graph's nodes)  \n",
    "\n",
    "The first step to take when facing one of the above problems is to find the correct way to represent the data and make them understandable for the model we are using.  \n",
    "In my experience, a traditional approach was just to use the graph modelling as a \"feature extractor\" for traditional machine learning models, adding predictors coming from network topology or interactions simulations.  \n",
    "  \n",
    "Anyway, to fully leverage the graph machine learning potentials, it is necessary to represent them in a way that is compatible with neural networks so that the graph structure itself becomes a source of information for the model. GNN are able to do that.  \n",
    "Machine Learning models are typically fed with regular arrays as input, so it is not trivial to find a way to represent a graph (with all the information levels mentioned above and the consequent interactions) for their training.  \n",
    "The adjacency matrix could be used, but it presents few problems (very well explained in <a href=\"https://youtu.be/JtDgmmQ60x8?t=411\" target=\"_blank\">this video</a> from the <a href=\"https://antoniolonga.github.io/Pytorch_geometric_tutorials/index.html\">Pytorch Geometric Tutorial project</a>[<sup>2</sup>](#fn2)):  \n",
    "- **sparsity** and consequent space inefficiency\n",
    "- it depends on nodes ordering and **it is not permutation invariant**, which means that many adjacency matrices can encode the same connectivity but obviously they would produce different results after being passed through a neural network\n",
    "- it does not handle a **graph change** in size: if a new node appears on the graph, the previously trained model becomes useless since it can not handle the new shape  \n",
    "\n",
    "This is why other steps like *message passing* and *aggregation* are taken to train GNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c228fd3-e09c-499a-b789-67df12489afd",
   "metadata": {},
   "source": [
    "<h2><strong>Computation Graphs, Message Passing and Aggregation</strong></h2> <a class=\"anchor\" id=\"computation-graphs\"></a>  \n",
    "\n",
    "The main intuition behind GNNs is that they manage to learn structural information about the graphs, based on the assumptions that neighboring nodes (nodes that are connected by an edge) share similar properties; therefore, **the computation graph of a node is indeed defined by its neighbors, and every node has its own computation graph** as shown in the example below.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"/images/computation_graph.png\" alt=\"Drawing\" width=600/> \n",
    "</div>\n",
    "\n",
    "The image shows what a Graph Neural Network with 3 different layers does to a single node.  \n",
    "Each layer can be seen as a progressive degree of distance from the target node:\n",
    "- Layer 0 contains the node features of the most distant neighbors (in this case neighbors at degree 2)\n",
    "- Layer 1 which is a sort of hidden layer that passes the aggregated information to node F (degree 1 distance)\n",
    "- Layer 2 which in this case is the final layer that returns the representation of node A\n",
    "\n",
    "From the image, we can see that both Layer 0 and Layer 1 include a neural network (usually a simple <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\" target=\"_blank\">Multi-layer perceptron</a> or a <a href=\"https://en.wikipedia.org/wiki/Recurrent_neural_network\" target=\"_blank\">Recurrent Neural Network</a>) executing the function $F(x_j) = \\mathbf{W}_j \\cdot x_j + b$.  \n",
    "In the article <a href=\"https://rish16.notion.site/Graph-Neural-Networks-for-Novice-Math-Fanatics-c51b922a595b4efd8647788475461d57\" target=\"_blank\"><em>Graph Neural Networks for Novice Math Fanatics</em></a>[<sup>3</sup>](#fn3), **message passing for a Graph Neural Network layer** is defined as: \n",
    ">**the process of taking node features of the neighbours, transforming them, and \"passing\" them to the source node. This process is repeated, in parallel, for all nodes in the graph. In that way, all neighbourhoods are examined by the end of this step.**\n",
    "\n",
    "Every layer terminates with an **aggregation step** that allows to perform an invariant operation (it does not depend on the nodes order) like *sum*, *average*, *min* or *max* before \"passing the message\".  \n",
    "When getting the target node, the aggregated messages are combined with the node's features, so that the embedded representation of the node containes the information coming from both the node's neighbors and the node itself with a simple addition or a concatenation.  \n",
    "For example, using addition and summarising the whole process to obtain the new representation $h_i$ of node $i$ with a formula:  \n",
    "  \n",
    "  \n",
    "$$\\large h_i = \\sigma(K(H(x_i) + \\bar{m}_i)))$$  \n",
    "\n",
    "where:\n",
    "- $x_i$ represents the features at node $i$, that will be combined with the aggregated messages\n",
    "- $m_i$ is the message aggregation \n",
    "- $H$ is the simple neural network mentioned before and applied on the node features\n",
    "- $K$ is another neural network used to project to another dimension both the node features and the aggregated messages (that in this case are summed together)\n",
    "- $\\sigma$ is an activation function like <a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\" target=\"_blank\"><em>Relu</em></a> or <a href=\"https://paperswithcode.com/method/leaky-relu#:~:text=Leaky%20Rectified%20Linear%20Unit%2C%20or,instead%20of%20a%20flat%20slope.&text=learnt%20during%20training.-,This%20type%20of%20activation%20function%20is%20popular%20in%20tasks%20where,example%20training%20generative%20adversarial%20networks.\" target=\"_blank\"><em>Leaky Relu</em></a>.\n",
    "  \n",
    "To conclude, a single Graph Neural Network layer applied on a single node $i$ and using addition, can be formulated as:  \n",
    "\n",
    "$$\\large h_i = \\sigma(W_1\\cdot h_i + \\sum_{j \\in \\mathcal{N}_i}\\mathbf{W}_2\\cdot h_j )$$\n",
    "  \n",
    "since it applies an activation on the sum of the features of $i$ multiplied by the weights at layer 1 ($W_1$) and the features of the neighbors $j$ multiplied by the weights at layer 2 ($W_2$).  \n",
    "The whole process obviously increases in complexity when given a full adjacency matrix to apply it on multiple nodes, but looking at a single node we can start getting the full picture.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f15a7e-0618-41cc-9e7e-6afd7f02da45",
   "metadata": {},
   "source": [
    "<h2><strong>Transductive and inductive learning</strong></h2> <a class=\"anchor\" id=\"transductive-inductive\"></a>  \n",
    "\n",
    "There are two main problem settings when dealing with GNN applications: **transductive** and **inductive** learning.  \n",
    "In the first, we usually deal with a single graph and are able to see both training and testing nodes (all of them belonging to the same graph), and also the structure and connections between them. When doing a transductive learning task what we do not see are, for example, the node labels that are the prediction targets (the CORA dataset mentioned above is a good example).  \n",
    "Doing inductive learning is quite different, since we cannot see test nodes during training and multiple graphs can be used: basically both training and testing happen on sets of graphs. This setting has been introduced with the paper <a href=\"https://arxiv.org/pdf/1706.02216.pdf\" target=\"_blank\"><em>Inductive Representation Learning on Large Graphs</em></a>[<sup>4</sup>](#fn4), that presented the GraphSAGE GNN framework.  \n",
    "Another interesting paper that deals with both inductive and transductive learning is <a href=\"https://arxiv.org/pdf/1710.10903.pdf\" target=\"_blank\"><em>Graph Attention Networks</em></a>[<sup>5</sup>](#fn5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830cea2-5afe-48a8-8057-703273a908da",
   "metadata": {},
   "source": [
    "<h2><strong>Frameworks for GNN</strong></h2> <a class=\"anchor\" id=\"frameworks\"></a>  \n",
    "\n",
    "GNNs are a great modelling tool for problems that can be represented through graphs, and within the python ecosystem there are several packages that can be used for their implementation both in Tensorflow and Pytorch.  \n",
    "The ones I came across with are mainly three:\n",
    "\n",
    "- <a href=\"https://pytorch-geometric.readthedocs.io/en/latest/\" target=\"_blank\"><strong>Pytorch Geometric</strong></a>  \n",
    "- <a href=\"https://graphneural.network/\" target=\"_blank\"><strong>Spektral</strong></a>\n",
    "- <a href=\"https://blog.tensorflow.org/2021/11/introducing-tensorflow-gnn.html\" target=\"_blank\"><strong>Tensorflow-gnn</strong></a>  \n",
    "\n",
    "In particular, I found <a href=\"https://arxiv.org/pdf/2006.12138.pdf\" target=\"_blank\">the Spektral project</a> particularly intuitive for getting started quicky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278809c-5721-4890-be83-ae6b5e0cd93f",
   "metadata": {},
   "source": [
    "<h2><strong>References</strong></h2> <a class=\"anchor\" id=\"references\"></a>\n",
    "\n",
    "<p id=\"fn1\">[1] Sanchez-Lengeling, et al., <a href=\"https://distill.pub/2021/gnn-intro/\">A Gentle Introduction to Graph Neural Networks</a>, Distill, 2021.</p>\n",
    "<p id=\"fn2\">[2] Longa A., Santin G., Pellegrini G., <a href=\"https://antoniolonga.github.io/Pytorch_geometric_tutorials/index.html\">Pytorch Geometric Tutorial</a>, 2021.</p>\n",
    "<p id=\"fn3\">[3] Anand R., <a href=\"https://rish16.notion.site/Graph-Neural-Networks-for-Novice-Math-Fanatics-c51b922a595b4efd8647788475461d57\">Graph Neural Networks for Novice Math Fanatics</a>, 2021.</p>\n",
    "<p id=\"fn4\">[4] Hamilton W. L., Ying R., Leskovec Y., <a href=\"https://arxiv.org/pdf/1706.02216.pdf\">Inductive Representation Learning on Large Graphs</a>, 2017.</p>\n",
    "<p id=\"fn5\">[5] Veličković P., et al., <a href=\"https://arxiv.org/pdf/1710.10903.pdf\">Graph Attention Networks</a>, 2018.</p>\n",
    "<p id=\"fn6\">[6] Grattarola D., Alippi C., <a href=\"https://arxiv.org/pdf/2006.12138.pdf\">Graph Neural Networks in TensorFlow and Keras with Spektral</a>, 2020.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nikola": {
   "category": "",
   "date": "2022-01-08 10:41:13 UTC",
   "description": "",
   "link": "",
   "slug": "a-quick-introduction-to-graph-neural-networks",
   "tags": "graph neural networks, deep learning",
   "title": "A quick introduction to Graph Neural Networks",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
